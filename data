setwd("C:/Users/volcano1/Downloads/haier")
Data <- read.csv("./data/huizong.csv", header = TRUE, encoding = 'utf-8')
colnames(Data) <- c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9")
index <- which(Data$x5 == "海尔")
haier_jd <- Data[index,6]
write.table(haier_jd, "./tmp/haier_jd.txt", row.names = FALSE)



setwd("C:/Users/volcano1/Downloads/haier")
Data <- readLines("./tmp/haier_jd.txt", encoding = "UTF-8")
length(Data)
#删除重复值
Data1 <- unique(Data)
length(Data1)


#数据预处理
setwd("C:/Users/volcano1/Downloads/haier")
# 读入数据
Data1 <- readLines("./data/haier_jd_process_end_负面情感结果.txt", encoding = "UTF-8")
Data2 <- readLines("./data/haier_jd_process_end_正面情感结果.txt", encoding = "UTF-8")
#删除前缀
for (i in 1:length(Data1)) {
  Data1[i] <- unlist(strsplit(Data1[i], "\\t"))[2]
}
for (i in 1:length(Data2)) {
  Data2[i] <- unlist(strsplit(Data2[i], "\\t"))[2]
}
write.table(Data1, "./tmp/haier_jd_neg.txt", row.names = FALSE)
write.table(Data2, "./tmp/haier_jd_pos.txt", row.names = FALSE)



#加载工作空间
library(jiebaRD) 
library(Rcpp) 
library(jiebaR) 
setwd("C:/Users/volcano1/Downloads/haier")
#对两个文本进行分词
Data1 <- readLines("./data/haier_jd_pos.txt", encoding = "UTF-8")
Data2 <- readLines("./data/haier_jd_neg.txt", encoding = "UTF-8")
cutter <- worker()
cutter <= Data1
cutter2 <- worker()
cutter2 <= Data2
write.table(cutter <= Data1, "./tmp/haier_jd_neg_cut.txt", row.names = FALSE)
write.table(cutter2 <= Data2, "./tmp/haier_jd_pos_cut.txt", row.names = FALSE)





# 加载工作空间
library(NLP)
library(tm)
library(slam)
library(wordcloud) 
library(topicmodels)
# 文本可视化与主题分析
setwd("C:/Users/volcano1/Downloads/haier")
Data1 <- readLines("./tmp/haier_jd_pos_cut.txt", encoding = "UTF-8")
Data2 <- readLines("./tmp/haier_jd_neg_cut.txt", encoding = "UTF-8")
stopwords <- unlist (readLines("./data/stoplist.txt", encoding = "UTF-8"))
# 删除stopwords

removeStopWords = function(x, words) {  
  ret <- character(0)
  index <- 1
  it_max <- length(x)
  while (index <= it_max) {
    if (length(words[words == x[index]]) <1) ret <- c(ret, x[index])
    index <- index + 1
  }
  ret
}

# 构建语料库
Data1 <- gsub("([a~z])", "", Data1)
Data2 <- gsub("([a~z])", "", Data2)

# 
sample.words1 <- lapply(Data1, removeStopWords, stopwords)
sample.words2 <- lapply(Data2, removeStopWords, stopwords)

# 构建语料库
corpus1 = Corpus(VectorSource(sample.words1))

# 建立文档  词条矩阵
sample.dtm1 <- DocumentTermMatrix(corpus1, control = list(wordLengths = c(2, Inf)))

# 主题模型分析
Gibbs = LDA(sample.dtm1, k = 3, method = "Gibbs", 
            control = list(seed = 2015, burnin = 1000, thin = 100, iter = 1000))
# 最可能主题文档
Topic1 <- topics(Gibbs, 1)
table(Topic1)
# 每个topic前十个term
Terms1 <- terms(Gibbs, 10)
Terms1

# 负面情绪LDA分析
# 构建语料库
corpus2 = Corpus(VectorSource(sample.words2))
# 建立文档  词条矩阵
sample.dtm2 <- DocumentTermMatrix(corpus2, control = list(wordLengths = c(2, Inf)))
# 主题模型分析
library(topicmodels)
Gibbs2 = LDA(sample.dtm2, k = 3, method = "Gibbs",
             control = list(seed = 2015, burnin = 1000, thin = 100, iter = 1000))
# 最可能主题文档
Topic2 <- topics(Gibbs2, 1)
table(Topic2)
# 每个topic前十个term
Terms2 <- terms(Gibbs2, 10)
Terms2
